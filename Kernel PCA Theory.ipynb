{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before explaining how Kernel PCA works, we first provide a brief introduction to PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel function and Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the problems where we have non-linear data structure, we apply a Kernel function to map the current space to a higher dimensional and linearly separable space. The Kernel function is non-linear and maps x to $\\phi{(x)}$. A kernel calculates the dot product of the kernel mapping of two data points:\n",
    "\n",
    "$$K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\phi\\left({x}_{{i}}\\right) \\phi\\left({x}_{{j}}\\right)^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let d denote the dimension of the original data before mapping, and D the dimension of the transformed data after kernel mapping. A simple example of the function and kernel can be:\n",
    "\n",
    "$$ {x}=\\left[\\begin{array}{ll}\n",
    "x_{1} & x_{2}\n",
    "\\end{array}\\right]^{T} \\quad {x} \\in \\mathbb{R}^{d} $$\n",
    "\n",
    "$$\n",
    "{\\phi{(x)}}=\\left[\\begin{array}{llllll}\n",
    "x_{1}^{2} & x_{1}x_{2} & {x}_{2}{x}_{1} & x_{2}^{2} \\end{array}\\right]^{T} \\quad {x} \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\phi\\left({x}_{{i}}\\right) \\phi\\left({x}_{{j}}\\right)^{T} = (<{x}_{i},{x}_{j}>)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some commonly used kernels, such as:\n",
    "\n",
    "(1). Gaussian radial basis function (RBF): \n",
    "$$\n",
    "K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\exp \\left(-\\gamma\\left\\|{x}_{{i}}-{x}_{{j}}\\right\\|_{2}^{2}\\right)\n",
    "$$\n",
    "\n",
    "(2). Polynomial function:\n",
    "\n",
    "$$\n",
    "K\\left({x}_{i}, {x}_{j}\\right)=\\left(1+{x}_{i} \\cdot {x}_{j}\\right)^{p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA Calculation & Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the eigen-decomposition of covariance matrix in PCA, we want to find the eigenvectors that correspond to the largest eigenvalues by applying the eigen-decomposition to the centered kernel matrix in Kernel PCA. \n",
    "\n",
    "The whole process can be devided into the following steps:\n",
    "\n",
    "(1) First we assume that the data has a zero mean:\n",
    "$$\\mu=\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right)=0$$ \n",
    "\n",
    "(2) Then we need the expression for the covariance matrix:\n",
    "\n",
    "According the definition of covariance matrix(as introduced in the Probabilistic modeling, inference and sampling part),\n",
    "\n",
    "$${C}_{{x},{x}}=\\mathbb{E}\\left[({x}-\\mathbb{E}[{x}])({x}-\\mathbb{E}[{x}])^{T}\\right]$$\n",
    "\n",
    "The covariance matrix can be estimated by:\n",
    "\n",
    "$$C=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{T}$$\n",
    "\n",
    "Since we assume that the data has zero mean ($\\bar{x} = 0$), and also apply a kernel function to x, we obtain the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathrm{C}=\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) \\phi\\left(x_{i}\\right)^{T} \\hspace{1cm}{(1)}$$ \n",
    "\n",
    "The eigen-decomposition of covariance matrix is:\n",
    "\n",
    "$$\\mathrm{C} v=\\lambda v  \\hspace{1cm}{(2)}$$ \n",
    "\n",
    "(3) Combine Eq.(1) and Eq.(2), we have:\n",
    "\n",
    "$$\\mathrm{C} v=\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) \\phi\\left(x_{i}\\right)^{T} v=\\lambda v \\hspace{1cm}{(3)}$$\n",
    "\n",
    "$$v=\\frac{1}{\\lambda n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) \\phi\\left(x_{i}\\right)^{T} v=\\frac{1}{\\lambda n} \\sum_{i=1}^{n}\\left(\\phi\\left(x_{i}\\right) \\cdot v\\right) \\phi\\left(x_{i}\\right)^{T}  \\hspace{1cm}{(4)}$$\n",
    "\n",
    "Let $\\alpha_{i} = \\frac{\\phi(x_{i})\\cdot v}{n \\cdot \\lambda}$, we have \n",
    "$$\n",
    "v=\\sum_{i=1}^{n} \\alpha_{i} \\phi\\left(x_{i}\\right) \\hspace{1cm}{(5)}\n",
    "$$\n",
    "\n",
    "Thus, we can see that the eigenvectors are the linear combinations of $\\phi(x_{i})$.\n",
    "\n",
    "(4) Subsitute the eigenvector in Eq.(3) by Eq.(5), we have:\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) \\phi\\left(x_{i}\\right)^{T}\\left(\\sum_{j=1}^{n} \\alpha^{k}_{j} \\phi\\left(x_{j}\\right)\\right)=\\lambda_{k} \\sum_{j=1}^{n} \\alpha^{k}_{j} \\phi\\left(x_{j}\\right)$$\n",
    "\n",
    "Then, since $ K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\phi\\left({x}_{{i}}\\right) \\phi\\left({x}_{{j}}\\right)^{T}$,\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right)\\left(\\sum_{j=1}^{n} \\alpha^{k}_{j} K\\left(x_{i}, x_{j}\\right)\\right)=\\lambda_{k} \\sum_{j=1}^{n} \\alpha^{k}_{j} \\phi\\left(x_{j}\\right)$$\n",
    "\n",
    "If we multiply both sides by $\\phi(x_{l})^{T}$:\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{l}\\right)^{T} \\phi\\left(x_{i}\\right)\\left(\\sum_{j=1}^{n} \\alpha^{k}_{j} K\\left(x_{i}, x_{j}\\right)\\right)=\\lambda_{k} \\sum_{j=1}^{n} \\alpha^{k}_{j} \\phi\\left(x_{l}\\right)^{T} \\phi\\left(x_{j}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, \n",
    "$$\n",
    "K^{2} \\alpha_{k} =n \\lambda_{k} K \\alpha_{k}  \n",
    "$$\n",
    "\n",
    "Simplify it, we have:\n",
    "\n",
    "$$ K \\alpha_{k} =n \\lambda_{k} \\alpha_{k}  \\hspace{1cm}{(6)} $$\n",
    "\n",
    "(5) We normalize the $\\alpha$ by normalizing the corresponding eigenvectors:\n",
    "\n",
    "$$v_{k}^{T} v_{k} = 1$$\n",
    "\n",
    "Plug in Eq.(5):\n",
    "$$\\sum_{i, j=1}^{n} \\alpha_{i}^{k} \\alpha_{j}^{k}\\left(\\phi\\left({x}_{i}\\right) \\cdot \\phi\\left({x}_{j}\\right)\\right)=\\left({\\alpha}^{k} {\\alpha}^{k}\\right ) \\cdot K ={\\alpha}^{k} \\cdot \\left(K {\\alpha}^{k} \\right)=n \\lambda_{k} \\alpha^{k} \\alpha^{k} = 1$$\n",
    "\n",
    "Note that the second to last equality holds by plugging in Eq.(6) derived in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) For any data point $x$, its projection into the eigenvector $v_j$ is:\n",
    "\n",
    "$$\n",
    "\\phi(x)^{T} v_{j}=\\sum_{i=1}^{n} \\alpha^{j}_{i} \\phi(x)^{T} \\phi\\left(x_{i}\\right)=\\sum_{i=1}^{n} \\alpha^{j}_{i} K\\left(x, x_{i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the first step, we assume that the data is centered at 0. For a general case, we need to normalize the data first by:\n",
    "\n",
    "$${\\phi}^{\\prime}\\left(x_{j}\\right)=\\phi\\left(x_{j}\\right)-\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the Kernel can be expressed by:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "{K}^{\\prime}\\left(x_{i}, x_{j}\\right)={\\phi}^{\\prime}\\left(x_{i}\\right) {\\phi}^{\\prime}\\left(x_{j}\\right) \\\\\n",
    "=\\left(\\phi\\left(x_{i}\\right)-\\frac{1}{n} \\sum_{k=1}^{n} \\phi\\left(x_{k}\\right)\\right)\\left(\\phi\\left(x_{j}\\right)-\\frac{1}{n} \\sum_{k=1}^{n} \\phi\\left(x_{k}\\right)\\right)  \\\\ \n",
    "=K\\left(x_{i}, x_{j}\\right)-\\frac{1}{n} \\sum_{k=1}^{n} K\\left(x_{i}, x_{k}\\right)-\\frac{1}{n} \\sum_{k=1}^{n} K\\left(x_{j}, x_{k}\\right)+\\frac{1}{n^{2}} \\sum_{l, k=1}^{n} K\\left(x_{l}, x_{k}\\right)\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "Simplify it, we have:\n",
    "\n",
    "$$\n",
    "K^{\\prime}=K-2 \\mathbf{1}_{\\mathrm{n}} K+\\mathbf{1}_{\\mathrm{n}} K{\\mathbf{1}}_{\\mathbf{n}} \\hspace{1cm}{(7)}\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{1}_{\\mathrm{n}} \\text { is a } n \\times n \\text { matrix with all values equal to } \\frac{1}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 Calculate the Kernel Matrix:\n",
    "\n",
    "For example, if Gaussian radial basis function (RBF) is used, then calculate the kernal matrix by:\n",
    "$$\n",
    "K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\exp \\left(-\\gamma\\left\\|{x}_{{i}}-{x}_{{j}}\\right\\|_{2}^{2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Normalize the Kernel Matrix:\n",
    "\n",
    "Use the Eq.(7) to calculate $K^{\\prime}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 Decompose the Normalized Kernel Matrix for Eigenvectors and Eigenvalues.\n",
    "\n",
    "According to Eq.(6), solve the following equation: \n",
    "\n",
    "$${\\mathrm{K}} \\alpha_{i}=\\lambda_{i} \\alpha_{i}$$\n",
    "\n",
    "Note\n",
    "(1) We can let the $\\lambda_{i} = n \\lambda_{j} \\text{in the Eq.(6)}$, which will not affect the following precedures of finding the largest eigenvalues since all eigenvalues are multiplied by n.\n",
    "(2) As indicated by Eq.(5), finding the eigenvectors is equivalent to finding the $\\alpha_{i}$s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "\n",
    "As stated in the Calculation & Derivation part, the projection into the eigenvector $v_j$ space of any data point $x$ is:\n",
    "\n",
    "$$\n",
    "\\phi(x)^{T} v_{j}=\\sum_{i=1}^{n} \\alpha^{j}_{i} K\\left(x, x_{i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
