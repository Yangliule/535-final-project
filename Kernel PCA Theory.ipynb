{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before explaining how Kernel PCA works, we first provide a brief introduction to PCA:\n",
    "\n",
    "Let d denote the dimension of the feature space, i.e. ${x} \\in \\mathbb{R}^{D} $, q denote the dimension of projection subspace. The principle components(PC) are the vectors that span the projection subspace. To find the PCs, we can maximize the variance or equivalently, minimize the mean squared error between projection vectors and originial vectors. This tutorial will follow the steps of variance maximization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1). To derive an expression for the variance, we first need the scalar projection of x, which is the length of vector projection, into the principal vector space. As we already learned from the class that the orthogonal projection of $v$ on an orthonormal list $\\left\\{\\mathbf{q}_{i}\\right\\}_{i=1}^{m}$ is $\\sum_{j=1}^{m}\\left\\langle\\mathbf{v}, \\mathbf{q}_{j}\\right\\rangle \\mathbf{q}_{j}$. Since principal directions span the projection subspace, they can be expressed as $\\left\\{\\mathbf{q}_{i}\\right\\}_{i=1}^{q}$. The scalar projection of x into ${q}_{i}$will be $\\left\\langle\\mathbf{v}, \\mathbf{q}_{i}\\right\\rangle$. We then can calculate the variance:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\sigma^{2} &=\\frac{1}{n} \\sum_{i}\\left(\\mathbf{x_{i}} \\cdot \\mathbf{q}\\right)^{2} \\\\\n",
    "&=\\frac{1}{n}(\\mathbf{x} \\mathbf{q})^{T}(\\mathbf{x} \\mathbf{q}) \\\\\n",
    "&=\\frac{1}{n} \\mathbf{q}^{T} \\mathbf{x}^{T} \\mathbf{x} \\mathbf{q} \\\\\n",
    "&=\\mathbf{q}^{T} \\frac{\\mathbf{x}^{T} \\mathbf{x}}{n} \\mathbf{q} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We assume that the data has zero mean(in the case where data is not centered at 0, we normalize it first), then $\\mathbf{x}^{T} \\mathbf{x} = {n}{C}$ and ${C}$ is the covariance matrix. Therefore, \n",
    "\n",
    "$$\n",
    "\\sigma^{2}=\\mathbf{q}^{T} {C} \\mathbf{q} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Maximize the variance by adding a Lagrange multiplier and taking derivatives to 0, we have:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{q}^{T} \\mathbf{q} &=1 \\\\\n",
    "{C} \\mathbf{q} &=\\lambda \\mathbf{q}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore, the principal directions are the eigenvectors of covariance matrix that correspond to the q largest eigenvalues. We can find the principal directions by direct eigen-decomposition, or by singular value decomposition(SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in class, any matrix has singular value decomposition, then ${x}$ can be written as:\n",
    "\n",
    "$$\n",
    "x= U \\Sigma V^{T}\n",
    "$$\n",
    "\n",
    "Note that ${U}$ and ${V}$ are orthonormal.\n",
    "\n",
    "Since ${C} = \\frac{\\mathbf{x}^{T} \\mathbf{x}}{n}$, the covariance matrix is symmetric and can be diagonalized:\n",
    "\n",
    "$${C}={V} {L} {V}^{T}$$\n",
    "\n",
    "where ${V}$ is the eigenvector and $L$ has eigenvalues on the diagonal with decreasing order.\n",
    "\n",
    "Plug in the SVD of ${x}$ to ${C}$, we have:\n",
    "\n",
    "$${C}=\\frac{(U \\Sigma V^{T})^{T} {U \\Sigma V}^{T}} {n}= \\frac{ V \\Sigma^{T} U^{T} {U^{T} \\Sigma V^{T}}} {n}={V} \\frac{{\\Sigma}^{2}}{n} {V}^{T}$$\n",
    "\n",
    "The right singular vectors ${V}$ are principal directions and the eigenvalues of covariance matrix are $\\lambda_{i}=\\frac{s_{i}^{2}}{n}$, where $s_{i}$ are the singular values. Principal components are given by ${x \\cdot V}={U \\Sigma V}^{T} {V}={U \\Sigma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel function and Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle the problems where we have non-linear data structure, we apply a Kernel function to map the current space to a higher dimensional and linearly separable space. The Kernel function is non-linear and maps x to $\\phi{(x)}$. A kernel calculates the dot product of the kernel mapping of two data points:\n",
    "\n",
    "$$K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\phi\\left({x}_{{i}}\\right) \\phi\\left({x}_{{j}}\\right)^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let d denote the dimension of the original data before mapping, and D the dimension of the transformed data after kernel mapping. A simple example of the function and kernel can be:\n",
    "\n",
    "$$ {x}=\\left[\\begin{array}{ll}\n",
    "x_{1} & x_{2}\n",
    "\\end{array}\\right]^{T} \\quad {x} \\in \\mathbb{R}^{d} $$\n",
    "\n",
    "$$\n",
    "{\\phi{(x)}}=\\left[\\begin{array}{llllll}\n",
    "x_{1}^{2} & x_{1}x_{2} & {x}_{2}{x}_{1} & x_{2}^{2} \\end{array}\\right]^{T} \\quad {x} \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\phi\\left({x}_{{i}}\\right) \\phi\\left({x}_{{j}}\\right)^{T} = (<{x}_{i},{x}_{j}>)^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some commonly used kernels, such as:\n",
    "\n",
    "(1). Gaussian radial basis function (RBF): \n",
    "$$\n",
    "K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\exp \\left(-\\gamma\\left\\|{x}_{{i}}-{x}_{{j}}\\right\\|_{2}^{2}\\right)\n",
    "$$\n",
    "\n",
    "(2). Polynomial function:\n",
    "\n",
    "$$\n",
    "K\\left({x}_{i}, {x}_{j}\\right)=\\left(1+{x}_{i} \\cdot {x}_{j}\\right)^{p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PC Calculation & Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the eigen-decomposition of covariance matrix in PCA, we want to find the eigenvectors that correspond to the largest eigenvalues by applying the eigen-decomposition to the centered kernel matrix in Kernel PCA. \n",
    "\n",
    "The whole process can be devided into the following steps:\n",
    "\n",
    "(1) First we assume that the data has a zero mean:\n",
    "$$\\mu=\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right)=0$$ \n",
    "\n",
    "(2) Then we need the expression for the covariance matrix:\n",
    "\n",
    "According the definition of covariance matrix(as introduced in the Probabilistic modeling, inference and sampling part),\n",
    "\n",
    "$${C}_{{x},{x}}=\\mathbb{E}\\left[({x}-\\mathbb{E}[{x}])({x}-\\mathbb{E}[{x}])^{T}\\right]$$\n",
    "\n",
    "The covariance matrix can be estimated by:\n",
    "\n",
    "$$C=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{T}$$\n",
    "\n",
    "Since we assume that the data has zero mean ($\\bar{x} = 0$), and also apply a kernel function to x, we obtain the following:\n",
    "\n",
    "$$\\mathrm{C}=\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) \\phi\\left(x_{i}\\right)^{T} \\hspace{1cm}{(1)}$$ \n",
    "\n",
    "The eigen-decomposition of covariance matrix is:\n",
    "\n",
    "$$\\mathrm{C} v=\\lambda v  \\hspace{1cm}{(2)}$$ \n",
    "\n",
    "(3) Combine Eq.(1) and Eq.(2), we have:\n",
    "\n",
    "$$\\mathrm{C} v=\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) \\phi\\left(x_{i}\\right)^{T} v=\\lambda v \\hspace{1cm}{(3)}$$\n",
    "\n",
    "$$v=\\frac{1}{\\lambda n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) \\phi\\left(x_{i}\\right)^{T} v=\\frac{1}{\\lambda n} \\sum_{i=1}^{n}\\left(\\phi\\left(x_{i}\\right) \\cdot v\\right) \\phi\\left(x_{i}\\right)^{T}  \\hspace{1cm}{(4)}$$\n",
    "\n",
    "Let $\\alpha_{i} = \\frac{\\phi(x_{i})\\cdot v}{n \\cdot \\lambda}$, we have \n",
    "$$\n",
    "v=\\sum_{i=1}^{n} \\alpha_{i} \\phi\\left(x_{i}\\right) \\hspace{1cm}{(5)}\n",
    "$$\n",
    "\n",
    "Thus, we can see that the eigenvectors are the linear combinations of $\\phi(x_{i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Subsitute the eigenvector in Eq.(3) by Eq.(5), we have:\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) \\phi\\left(x_{i}\\right)^{T}\\left(\\sum_{j=1}^{n} \\alpha^{k}_{j} \\phi\\left(x_{j}\\right)\\right)=\\lambda_{k} \\sum_{j=1}^{n} \\alpha^{k}_{j} \\phi\\left(x_{j}\\right)$$\n",
    "\n",
    "Then, since $ K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\phi\\left({x}_{{i}}\\right) \\phi\\left({x}_{{j}}\\right)^{T}$,\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right)\\left(\\sum_{j=1}^{n} \\alpha^{k}_{j} K\\left(x_{i}, x_{j}\\right)\\right)=\\lambda_{k} \\sum_{j=1}^{n} \\alpha^{k}_{j} \\phi\\left(x_{j}\\right)$$\n",
    "\n",
    "If we multiply both sides by $\\phi(x_{l})^{T}$:\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{l}\\right)^{T} \\phi\\left(x_{i}\\right)\\left(\\sum_{j=1}^{n} \\alpha^{k}_{j} K\\left(x_{i}, x_{j}\\right)\\right)=\\lambda_{k} \\sum_{j=1}^{n} \\alpha^{k}_{j} \\phi\\left(x_{l}\\right)^{T} \\phi\\left(x_{j}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, \n",
    "$$\n",
    "K^{2} \\alpha_{k} =n \\lambda_{k} K \\alpha_{k}  \n",
    "$$\n",
    "\n",
    "Simplify it, we have:\n",
    "\n",
    "$$ K \\alpha_{k} =n \\lambda_{k} \\alpha_{k}  \\hspace{1cm}{(6)} $$\n",
    "\n",
    "(5) We normalize the $\\alpha$ by normalizing the corresponding eigenvectors:\n",
    "\n",
    "$$v_{k}^{T} v_{k} = 1$$\n",
    "\n",
    "Plug in Eq.(5):\n",
    "$$\\sum_{i, j=1}^{n} \\alpha_{i}^{k} \\alpha_{j}^{k}\\left(\\phi\\left({x}_{i}\\right) \\cdot \\phi\\left({x}_{j}\\right)\\right)=\\left({\\alpha}^{k} {\\alpha}^{k}\\right ) \\cdot K ={\\alpha}^{k} \\cdot \\left(K {\\alpha}^{k} \\right)=n \\lambda_{k} \\alpha^{k} \\alpha^{k} = 1$$\n",
    "\n",
    "Note that the second to last equality holds by plugging in Eq.(6) derived in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) For any data point $x$, its projection into the eigenvector $v_j$ is:\n",
    "\n",
    "$$\n",
    "\\phi(x)^{T} v_{j}=\\sum_{i=1}^{n} \\alpha^{j}_{i} \\phi(x)^{T} \\phi\\left(x_{i}\\right)=\\sum_{i=1}^{n} \\alpha^{j}_{i} K\\left(x, x_{i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the first step, we assume that the data is centered at 0. For a general case, we need to normalize the data first by:\n",
    "\n",
    "$${\\phi}^{\\prime}\\left(x_{j}\\right)=\\phi\\left(x_{j}\\right)-\\frac{1}{n} \\sum_{i=1}^{n} \\phi\\left(x_{i}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the Kernel can be expressed by:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "{K}^{\\prime}\\left(x_{i}, x_{j}\\right)={\\phi}^{\\prime}\\left(x_{i}\\right) {\\phi}^{\\prime}\\left(x_{j}\\right) \\\\\n",
    "=\\left(\\phi\\left(x_{i}\\right)-\\frac{1}{n} \\sum_{k=1}^{n} \\phi\\left(x_{k}\\right)\\right)\\left(\\phi\\left(x_{j}\\right)-\\frac{1}{n} \\sum_{k=1}^{n} \\phi\\left(x_{k}\\right)\\right)  \\\\ \n",
    "=K\\left(x_{i}, x_{j}\\right)-\\frac{1}{n} \\sum_{k=1}^{n} K\\left(x_{i}, x_{k}\\right)-\\frac{1}{n} \\sum_{k=1}^{n} K\\left(x_{j}, x_{k}\\right)+\\frac{1}{n^{2}} \\sum_{l, k=1}^{n} K\\left(x_{l}, x_{k}\\right)\n",
    "\\end{array} \n",
    "$$\n",
    "\n",
    "Simplify it, we have:\n",
    "\n",
    "$$\n",
    "K^{\\prime}=K-2 \\mathbf{1}_{\\mathrm{n}} K+\\mathbf{1}_{\\mathrm{n}} K{\\mathbf{1}}_{\\mathbf{n}} \\hspace{1cm}{(7)}\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{1}_{\\mathrm{n}} \\text { is a } n \\times n \\text { matrix with all values equal to } \\frac{1}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 Calculate the Kernel Matrix:\n",
    "\n",
    "For example, if Gaussian radial basis function (RBF) is used, then calculate the kernal matrix by:\n",
    "$$\n",
    "K\\left({x}_{{i}}, {x}_{{j}}\\right)=\\exp \\left(-\\gamma\\left\\|{x}_{{i}}-{x}_{{j}}\\right\\|_{2}^{2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 Normalize the Kernel Matrix:\n",
    "\n",
    "Use the Eq.(7) to calculate $K^{\\prime}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 Decompose the Normalized Kernel Matrix for Eigenvectors and Eigenvalues.\n",
    "\n",
    "According to Eq.(6), solve the following equation: \n",
    "\n",
    "$${\\mathrm{K}} \\alpha_{i}=\\lambda_{i} \\alpha_{i}$$\n",
    "\n",
    "Note\n",
    "(1) We can let the $\\lambda_{i} = n \\lambda_{j} \\text{in the Eq.(6)}$, which will not affect the following precedures of finding the largest eigenvalues since all eigenvalues are multiplied by n.\n",
    "(2) As indicated by Eq.(5), finding the eigenvectors is equivalent to finding the $\\alpha_{i}$s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4\n",
    "\n",
    "As stated in the Calculation & Derivation part, the projection into the eigenvector $v_j$ space of any data point $x$ is:\n",
    "\n",
    "$$\n",
    "\\phi(x)^{T} v_{j}=\\sum_{i=1}^{n} \\alpha^{j}_{i} K\\left(x, x_{i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
