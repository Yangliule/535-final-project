{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "535 Final Project.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjFIBLd_kr9X"
      },
      "source": [
        "## Kernel PCA\n",
        "#### Team members: Liule Yang, Zhiwen Xu, Yuxuan Liu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0jf7DxRkr9g"
      },
      "source": [
        "## Background and motivation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XMq9DFlkr9h"
      },
      "source": [
        "Principal component analysis (PCA) is one of the most popular dimensionality reduction and data visualization right now (Bishop, 561). PCA can extract linear features of a matrix input data and some meaningful analysis can be performed based on that. PCA can be performed through eigen-decomposition as well as singular value decomposition (SVD). PCA works well in some cases, for example, when dealing with DNA sequences, it can achieve satisfactory results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEukVZ5akr9i"
      },
      "source": [
        "However, PCA can only extract linear features. When the data is not linearly separable, we need to do a kernel trick and here we need kernel PCA (KPCA). With the KPCA, we can extract nonlinear features. Common kernels for KPCA include polynomial kernel, Gaussian kernel, and Laplace kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1JNFqRRkr9j"
      },
      "source": [
        "## Theory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmgIlVqWkr9l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfRBezUbkr9o"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQP3zvjCzEZ7"
      },
      "source": [
        "**Gaussian radial basis function (RBF) Kernel PCA**\r\n",
        "\r\n",
        "This is the implementation of the Gaussian RBF kernel. This function takes an MxN data matrix and returns an MxM kernel matrix mapped by the kernel function. It first calculates the pairwise Euclidean distances of data points and transforms the result into an MxM symmetric matrix. Then it applies the RBF kernel function over this matrix and outputs the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BorVsQprygU8"
      },
      "source": [
        "from numpy import exp\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "def gaussian_kernel(X, gamma):\r\n",
        "    # calculate the square of the Euclidean distance for each pair of points\r\n",
        "    distances = []\r\n",
        "    for i in range(0,len(X)-1):\r\n",
        "      for j in range(i+1,len(X)):\r\n",
        "        distances.append(pow(X[i][0] - X[j][0], 2) + pow(X[i][1] - X[j][1], 2))\r\n",
        "    # convert the squared distances in to the form of a symmetric matrix of dimension MxM\r\n",
        "    s = (len(X),len(X))\r\n",
        "    square = np.zeros(s)\r\n",
        "    idx = 0\r\n",
        "    for i in range(0,len(X)):\r\n",
        "      for j in range(i+1,len(X)):\r\n",
        "        square[i][j] = distances[idx]\r\n",
        "        square[j][i] = distances[idx]\r\n",
        "        idx += 1\r\n",
        "    # calculated the MxM kernel matrix using the Gaussian RBF kernel function\r\n",
        "    K = exp(-gamma * square)\r\n",
        "    return K"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVqwr62TPM1G"
      },
      "source": [
        "**Polynomial Kernel PCA**\r\n",
        "\r\n",
        "This is the implementation of the Polynomial kernel. This function takes an MxN data matrix and returns an MxM kernel matrix mapped by the kernel function. It first calculates the pairwise dot products data points and transforms the result into an MxM symmetric matrix. Then it applies the polynomial kernel function over this matrix and outputs the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkyPsx0COElM"
      },
      "source": [
        "def poly_kernel(X, p):\r\n",
        "    # calculate the dot product for each pair of points\r\n",
        "    products = []\r\n",
        "    for i in range(0,len(X)-1):\r\n",
        "      for j in range(i+1,len(X)):\r\n",
        "        products.append(X[i] @ X[j])\r\n",
        "    # convert the squared distances in to the form of a symmetric matrix of dimension MxM\r\n",
        "    s = (len(X),len(X))\r\n",
        "    square = np.zeros(s)\r\n",
        "    idx = 0\r\n",
        "    for i in range(0,len(X)):\r\n",
        "      for j in range(i+1,len(X)):\r\n",
        "        square[i][j] = products[idx]\r\n",
        "        square[j][i] = products[idx]\r\n",
        "        idx += 1\r\n",
        "    # calculated the MxM kernel matrix using the polynomial kernel function\r\n",
        "    K = pow(1 + square, p)\r\n",
        "    return K"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv9WDXh4PNYo"
      },
      "source": [
        "**Sigmoid Kernel PCA**\r\n",
        "\r\n",
        "This is the implementation of the Sigmoid kernel. This function takes an MxN data matrix and returns an MxM kernel matrix mapped by the kernel function. It first calculates the pairwise dot products data points and transforms the result into an MxM symmetric matrix. Then it applies the sigmoid kernel function over this matrix and outputs the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA3c2kivOEO6"
      },
      "source": [
        "def sigmoid_kernel(X, sigma):\r\n",
        "    # calculate the dot product for each pair of points\r\n",
        "    products = []\r\n",
        "    for i in range(0,len(X)-1):\r\n",
        "      for j in range(i+1,len(X)):\r\n",
        "        products.append(X[i] @ X[j])\r\n",
        "    # convert the squared distances in to the form of a symmetric matrix of dimension MxM\r\n",
        "    s = (len(X),len(X))\r\n",
        "    square = np.zeros(s)\r\n",
        "    idx = 0\r\n",
        "    for i in range(0,len(X)):\r\n",
        "      for j in range(i+1,len(X)):\r\n",
        "        square[i][j] = products[idx]\r\n",
        "        square[j][i] = products[idx]\r\n",
        "        idx += 1\r\n",
        "    # calculated the MxM kernel matrix using the sigmoid kernel function\r\n",
        "    square = square + sigma\r\n",
        "    for i in range(0,len(X)):\r\n",
        "      for j in range(0,len(X)):\r\n",
        "        square[i][j] = math.tan(square[i][j])\r\n",
        "    K = square\r\n",
        "    return K"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJnBzGwAkr9p"
      },
      "source": [
        "#### The eigen decomposition implementation of PCA\n",
        "\n",
        "The first type of algorithm is utilizing the information from eigenvalues and eigenvectors. First, the input matrix will be standardized by deducting mean of the matrix. Then, the covariance matrix of the input matrix will be computed, and the eigenvector and eigenvalue will be computed based on the covariance matrix. Then, sort the eigenvectors based on eigenvalues from large to small, and then select the top n components to make transformation of the original input matrix to a matrix with n features (n columns)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqs6BAW7kr9r"
      },
      "source": [
        "def eigen_pca(m, n_components = 2):\n",
        "    # compute the mean and standarize the input matrix\n",
        "    mean = np.mean(m,0)\n",
        "    centered_m = m - mean\n",
        "    num_rows, num_cols = centered_m.shape\n",
        "    # compute covariance matrix\n",
        "    cov_matrix = np.cov(np.transpose(centered_m))\n",
        "    # compute eigenvector and eigenvalue\n",
        "    eig_val_cov, eig_vec_cov = np.linalg.eig(cov_matrix)\n",
        "    # sort the eigenvalue and eigenvector pair by the order of eigenvector from large to small\n",
        "    eig_pairs = [(np.abs(eig_val_cov[i]), eig_vec_cov[:,i]) for i in range(len(eig_val_cov))]\n",
        "    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "    # transform to the reduced dimension\n",
        "    matrix_w = np.hstack((eig_pairs[k][1].reshape(num_cols,1) for k in range(n_components)))\n",
        "    transformed = matrix_w.T.dot(centered_m.transpose())\n",
        "    return transformed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_lGE4Dakr9s"
      },
      "source": [
        "#### The SVD implementation of PCA\n",
        "\n",
        "The PCA can also be performed using SVD. The only difference is that we are using sigular value this time instead of eigenvalues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfOOhF_Ukr9s"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "def svd_pca(m):\n",
        "    # compute the mean and standarize the input matrix\n",
        "    mean = np.mean(m,0)\n",
        "    centered_m = m - mean\n",
        "    num_rows, num_cols = centered_m.shape\n",
        "    # compute covariance matrix\n",
        "    cov_matrix = np.cov(np.transpose(centered_m))\n",
        "    # compute eigenvector and eigenvalue\n",
        "    eig_val_cov, eig_vec_cov = np.linalg.eig(cov_matrix)\n",
        "    # sort the eigenvalue and eigenvector pair by the order of eigenvector from large to small\n",
        "    eig_pairs = [(np.abs(eig_val_cov[i]), eig_vec_cov[:,i]) for i in range(len(eig_val_cov))]\n",
        "    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "    # transform to the reduced dimension\n",
        "    matrix_w = np.hstack((eig_pairs[k][1].reshape(num_cols,1) for k in range(n_components)))\n",
        "    transformed = matrix_w.T.dot(centered_m.transpose())\n",
        "    return transformed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTgW9ShBkr9u"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76OfXNg4kr9v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oz9Q_ZJkr9v"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc3tRN0Ekr9w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}